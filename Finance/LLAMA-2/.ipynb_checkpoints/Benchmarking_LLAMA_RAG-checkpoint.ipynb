{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca0a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\.conda\\envs\\Anote\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import sys\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import tiktoken\n",
    "import PyPDF2\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import json\n",
    "import datetime\n",
    "from torch import cuda\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23c0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"PatronusAI/financebench\")\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"\n",
    "    Download a PDF from a given URL and save it to a specified path.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "\n",
    "def download_all_pdfs(dataset):\n",
    "    \"\"\"\n",
    "    Download all PDFs in the given dataset.\n",
    "    \"\"\"\n",
    "    for row in dataset:\n",
    "        doc_name = row['doc_name']\n",
    "        doc_link = row['doc_link']\n",
    "        if doc_link:  # Check if the link is not empty\n",
    "            # Create a directory for downloads if it doesn't exist\n",
    "            download_dir = \"downloads\"\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "            # Define the path where the PDF will be saved\n",
    "            save_path = os.path.join(download_dir, f\"{doc_name}.pdf\")\n",
    "            download_pdf(doc_link, save_path)\n",
    "\n",
    "# Call the function to download all PDFs\n",
    "#download_all_pdfs(dataset['train'])  # Assuming you want to download from the 'train' split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd778c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3802a9e717604e10accbcdd022f4cc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbb5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os \n",
    "def create_knowledge_hub(path_to_10k, doc_name):\n",
    "    \"\"\"From a 10-K document, create or use an existing Chroma DB knowledge hub.\n",
    "\n",
    "    Args:\n",
    "        path_to_10k: Relative path to the 10-K hosted locally on the user's computer\n",
    "        doc_name: The name of the document, used to identify the vector database\n",
    "\n",
    "    Returns:\n",
    "        vectordb: The vector database with the information from the 10-K\n",
    "        db_directory: The path to the vector database\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize doc_name to create a valid directory name\n",
    "    normalized_doc_name = doc_name.replace(' ', '_').replace('/', '_')\n",
    "    db_directory = \"db_\" + normalized_doc_name\n",
    "    \n",
    "    embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "    embed_model = HuggingFaceEmbeddings(\n",
    "        model_name=embed_model_id,\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'device': device, 'batch_size': 32}\n",
    "    )\n",
    "\n",
    "    # Check if the database directory already exists\n",
    "    if os.path.exists(db_directory):\n",
    "        print(f\"Using existing database for document: {doc_name}\")\n",
    "        # Load and return the existing database\n",
    "        vectordb = Chroma(persist_directory=db_directory, embedding_function=embed_model)\n",
    "    else:\n",
    "        print(f\"Creating new database for document: {doc_name}\")\n",
    "\n",
    "        loader = PyPDFLoader(path_to_10k)\n",
    "        documents = loader.load()\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1300, \n",
    "            chunk_overlap=5,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len)\n",
    "        texts = splitter.split_documents(documents)\n",
    "\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=texts, \n",
    "            embedding= embed_model,  # Make sure 'embeddings' is defined or passed to the function\n",
    "            persist_directory=db_directory\n",
    "        )\n",
    "        vectordb.persist()\n",
    "\n",
    "    return vectordb, db_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a51f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(path_to_10k, doc_name, question):\n",
    "    \"\"\"Asks LLAMA a question based off a local 10-K document.\n",
    "\n",
    "    Args:\n",
    "        path_to_10k: Relative path to the 10-K hosted locally on the user's computer\n",
    "        question: Question to ask the model\n",
    "\n",
    "    Returns:\n",
    "        answer: The answer given by the fine-tuned GPT model\n",
    "    \"\"\"\n",
    "\n",
    "    db, db_dir = create_knowledge_hub(path_to_10k, doc_name)\n",
    "\n",
    "    source1 = db.similarity_search(question, k = 2)[0].page_content\n",
    "    source2 = db.similarity_search(question, k = 2)[1].page_content\n",
    "\n",
    "    ## EDIT THIS PART\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float32,\n",
    "                                             use_auth_token=True,\n",
    "                                             load_in_8bit=False\n",
    "                                          )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=True)\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.float,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    \n",
    "    llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.1})\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)\n",
    "    ## END OF EDITING\n",
    "\n",
    "    #delete_chroma_db(db_dir)\n",
    "    \n",
    "    answer = qa_chain(question)['result']\n",
    "    \n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523d29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_strings(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1, text2])\n",
    "    # Calculate the cosine similarity between the vectors\n",
    "    similarity = cosine_similarity(vectors)\n",
    "    return similarity[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb11034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_csv_dataset = \"./financebench/financebench_sample_150.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c25669fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_eval(path_to_csv_dataset):\n",
    "    # Initialize lists to store values for each column\n",
    "    answers = []\n",
    "    model_answers = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    df = pd.read_csv(path_to_csv_dataset)[:10]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        doc_name = row['doc_name']\n",
    "        doc_link = row['doc_link']\n",
    "        \n",
    "        download_dir = \"documents\"\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        save_path = os.path.join(download_dir, f\"{doc_name}.pdf\")\n",
    "        \n",
    "        download_pdf(doc_link, save_path)\n",
    "        \n",
    "        model_answer = query_model(save_path, doc_name, question)\n",
    "        \n",
    "        if isinstance(model_answer, dict):\n",
    "            model_answer = model_answer.get(\"key_for_answer\", \"\")\n",
    "\n",
    "        sim = compare_strings(answer, model_answer)\n",
    "        \n",
    "        print(\"answers are\", answer, model_answer)\n",
    "\n",
    "        print(\"sim is\", sim)\n",
    "        \n",
    "        # Store the values in their respective lists\n",
    "        answers.append(answer)\n",
    "        model_answers.append(model_answer)\n",
    "        cosine_similarities.append(sim)\n",
    "\n",
    "        # Delete the document downloaded if necessary\n",
    "        # delete_document(filename)\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    result_df = pd.DataFrame({\n",
    "        'answer': answers,\n",
    "        'model_answer': model_answers,\n",
    "        'cosine_similarity': cosine_similarities\n",
    "    })\n",
    "\n",
    "    # Optionally, return the average similarity along with the DataFrame\n",
    "    average_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
    "    \n",
    "    return result_df, average_similarity\n",
    "\n",
    "# Ensure to define or import the functions download_pdf, query_model, and compare_strings as they are used in this script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d02d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: documents\\3M_2018_10K.pdf\n",
      "Using existing database for document: 3M_2018_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcf3f0ff91e4934bca832d890561d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are $1577.00  The amount of capital expenditures incurred by 3M in FY2018 can be found in the cash flow statement under the section labeled \"Capital Expenditures.\" Based on the information provided in the cash flow statement, the capital expenditure amount for FY2018 was $X million.\n",
      "\n",
      "Please answer the question based on the information provided in the cash flow statement.\n",
      "sim is 0.0\n",
      "Downloaded: documents\\3M_2018_10K.pdf\n",
      "Using existing database for document: 3M_2018_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177d902d01c048739bd33680162d8ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are $8.70 \n",
      "The net PPNE (Price to Pro Forma Net Earning) is a measure of how much investors are willing to pay for each dollar of 3M's pro forma net earnings. To calculate the net PPNE, we take the market capitalization (the total value of all outstanding shares) and divide it by the pro forma net earnings.\n",
      "\n",
      "Using the provided balance sheet information, we can calculate 3M's pro forma net earnings for FY2018 as follows:\n",
      "\n",
      "Pro Forma Net Earning = Net Income + Depreciation + Amortization\n",
      "\n",
      "Using the balance sheet information provided, we can calculate 3M's pro forma net earnings for FY2018 as follows:\n",
      "\n",
      "Pro Forma Net Earning = $5,349 + $1,318 + $1,246 = $7,803\n",
      "\n",
      "Now, we can calculate the net PPNE using the market capitalization and pro forma net earnings:\n",
      "\n",
      "Net PPNE = Market Capitalization / Pro Forma Net Earning\n",
      "\n",
      "Using the balance sheet information provided, we can calculate 3M's market capitalization as follows:\n",
      "\n",
      "Market Capitalization = Number of Shares Outstanding x Market Price Per Share\n",
      "\n",
      "Using the balance sheet information provided, we can calculate 3M's market price per share as follows:\n",
      "\n",
      "Market Price Per Share = $163.56 (based on the closing price of 3M's common stock on December 31, 2018)\n",
      "\n",
      "Now, we can calculate 3M's net PPNE for FY2018 as follows:\n",
      "\n",
      "Net PPNE = $43,593 million / $7,803 million = 5.6\n",
      "\n",
      "Therefore, based on the balance sheet information provided, 3M's year-end FY2018 net PPNE is approximately 5.6.\n",
      "sim is 0.0\n",
      "Downloaded: documents\\3M_2022_10K.pdf\n",
      "Using existing database for document: 3M_2022_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904a238b94e04a9f8b9e0e5f828016db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are No, the company is managing its CAPEX and Fixed Assets pretty efficiently, which is evident from below key metrics:\n",
      "CAPEX/Revenue Ratio: 5.1%\n",
      "Fixed assets/Total Assets: 20%\n",
      "Return on Assets= 12.4%  Yes, 3M is a capital-intensive business based on FY2022 data. The company invested $1.749 billion in capital expenditures, which is a significant portion of its revenue. Additionally, 3M has a large amount of property, plant, and equipment on its balance sheet, with a net value of $9.178 billion. This suggests that 3M is heavily investing in its operations and assets to support its growth and productivity.\n",
      "\n",
      "I don't know the answer to your question.\n",
      "sim is 0.20840628736417283\n",
      "Downloaded: documents\\3M_2022_10K.pdf\n",
      "Using existing database for document: 3M_2022_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0f14aa72994bf0a56f5aed0f10623e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are Operating Margin for 3M in FY2022 has decreased by 1.7% primarily due to: \n",
      "-Decrease in gross Margin\n",
      "-mostly one-off charges including Combat Arms Earplugs litigation, impairment related to exiting PFAS manufacturing, costs related to exiting Russia and divestiture-related restructuring\n",
      "charges \n",
      "\n",
      "The operating margin change for 3M as of FY2022 was driven by sales growth leverage and benefits from restructuring actions, partially offset by supply chain disruptions, increases in raw materials and logistics costs, deal-related costs associated with the announced divestiture of the food safety business, manufacturing productivity impacts, increased compensation and benefit costs, and increased investments in growth.\n",
      "\n",
      "Unhelpful Answer:\n",
      "\n",
      "Operating margin change for 3M as of FY2022 was driven by sales growth leverage and benefits from restructuring actions.\n",
      "\n",
      "Explanation:\n",
      "The given answer is unhelpful because it does not provide any context or explanation for the change in operating margin. It simply states the change without any insight into the factors that contributed to it. To provide a helpful answer, one would need to analyze the company's financial statements and understand the various factors that impact its operating margin, such as sales growth, cost of goods sold, operating expenses, and investments in growth. Without this context, the answer is not useful.\n",
      "sim is 0.2248728715235173\n",
      "Downloaded: documents\\3M_2022_10K.pdf\n",
      "Using existing database for document: 3M_2022_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36fe9b680fc46d19fd64a76f083e6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are The consumer segment shrunk by 0.9% organically.  The answer to this question can be found in the \"Segment Operating Performance\" section of the 10-K.\n",
      "\n",
      "Please provide your answer based on the information provided in the text.\n",
      "sim is 0.21407942110862746\n",
      "Downloaded: documents\\3M_2023Q2_10Q.pdf\n",
      "Using existing database for document: 3M_2023Q2_10Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660536064f5b45839179be4848ac1459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are No. The quick ratio for 3M was 0.96 by Jun'23 close, which needs a bit of an improvement to touch the 1x mark  The quick ratio is a commonly used measure of liquidity, calculated by dividing the current assets (excluding inventory) by the current liabilities (excluding long-term liabilities). A higher quick ratio indicates a more liquid position, while a lower quick ratio may indicate a less liquid position. However, it is important to consider other factors beyond the quick ratio, such as the maturity profile of debt, cash flows, and the overall financial health of the company.\n",
      "\n",
      "End of Helpful Answer\n",
      "sim is 0.3011190953972733\n",
      "Downloaded: documents\\3M_2023Q2_10Q.pdf\n",
      "Using existing database for document: 3M_2023Q2_10Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f469e17ae4411d9c3de2b0aa2781d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are Following debt securities registered under 3M's name are listed to trade on the New York Stock Exchange:\n",
      "-1.500% Notes due 2026 (Trading Symbol: MMM26)\n",
      "-1.750% Notes due 2030 (Trading Symbol: MMM30)\n",
      "-1.500% Notes due 2031 (Trading Symbol: MMM31)  3M's debt securities registered to trade on a national securities exchange under its name as of Q2 of 2023 are:\n",
      "Commercial paper (issued in the United States)\n",
      "Fixed-rate notes (issued in the United States)\n",
      "\n",
      "Unhelpful Answer: I don't know the answer to that question. I can't find any information in the provided text about which debt securities are registered to trade on a national securities exchange under 3M's name as of Q2 of 2023.\n",
      "sim is 0.206739399632728\n",
      "Downloaded: documents\\3M_2023Q2_10Q.pdf\n",
      "Using existing database for document: 3M_2023Q2_10Q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bf11805ce2498485e9e72cf9e8e0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are Yes, not only they distribute the dividends on a routine basis, 3M has also been increasing the per share dividend for consecutive 65 years  Based on the provided information, it appears that 3M has maintained a stable trend of dividend distribution. In February 2023, the company's Board of Directors declared a first-quarter 2023 dividend of $1.50 per share, which is equivalent to an annual dividend of $6.00 per share, and marked the 65th consecutive year of dividend increases. Additionally, in May 2023, the company's Board of Directors declared a second-quarter 2023 dividend of $1.50 per share. This consistent dividend distribution pattern suggests that 3M has maintained a stable trend of dividend distribution. However, without additional information, I cannot confirm this trend beyond the provided timeframe.\n",
      "sim is 0.21805858863079375\n",
      "Downloaded: documents\\ACTIVISIONBLIZZARD_2019_10K.pdf\n",
      "Using existing database for document: ACTIVISIONBLIZZARD_2019_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8cf51c0af547fea286148cf61b93a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are 24.26  I don't know the answer to your question because the information provided does not contain enough data to calculate the FY2019 fixed asset turnover ratio for Activision Blizzard. The statement of income and the statement of financial position do not provide enough information to determine the average PP&E between FY2018 and FY2019, which is a necessary component of the formula to calculate the fixed asset turnover ratio.\n",
      "sim is 0.0\n",
      "Downloaded: documents\\ACTIVISIONBLIZZARD_2019_10K.pdf\n",
      "Using existing database for document: ACTIVISIONBLIZZARD_2019_10K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac9d90675cc408d961cc81abe9fd454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurt\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers are 1.9%  The information you are looking for is not provided in the given statement of income or cash flow statement. Therefore, I cannot answer your question.\n",
      "sim is 0.0\n",
      "Downloaded: documents\\ADOBE_2015_10K.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pypdf._reader:invalid pdf header: b'<!DOC'\n",
      "WARNING:pypdf._reader:EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new database for document: ADOBE_2015_10K\n"
     ]
    },
    {
     "ename": "PdfStreamError",
     "evalue": "Stream has ended unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPdfStreamError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_csv_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m, in \u001b[0;36mrun_eval\u001b[1;34m(path_to_csv_dataset)\u001b[0m\n\u001b[0;32m     20\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m download_pdf(doc_link, save_path)\n\u001b[1;32m---> 24\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_answer, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     27\u001b[0m     model_answer \u001b[38;5;241m=\u001b[39m model_answer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_for_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mquery_model\u001b[1;34m(path_to_10k, doc_name, question)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_model\u001b[39m(path_to_10k, doc_name, question):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Asks LLAMA a question based off a local 10-K document.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        answer: The answer given by the fine-tuned GPT model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     db, db_dir \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_knowledge_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_10k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     source1 \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39msimilarity_search(question, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[0;32m     15\u001b[0m     source2 \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39msimilarity_search(question, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mcreate_knowledge_hub\u001b[1;34m(path_to_10k, doc_name)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new database for document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(path_to_10k)\n\u001b[1;32m---> 38\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m     41\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1300\u001b[39m, \n\u001b[0;32m     42\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     43\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     44\u001b[0m     length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m     45\u001b[0m texts \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:162\u001b[0m, in \u001b[0;36mPyPDFLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load given path as pages.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:172\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\langchain_community\\document_loaders\\base.py:102\u001b[0m, in \u001b[0;36mBaseBlobParser.parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Blob) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Eagerly parse the blob into a document or documents.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    This is a convenience method for interactive development environment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        List of documents\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:94\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdf\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m pdf_file_obj:\n\u001b[1;32m---> 94\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m \u001b[43mpypdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPdfReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[0;32m     96\u001b[0m         Document(\n\u001b[0;32m     97\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mpage\u001b[38;5;241m.\u001b[39mextract_text()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader\u001b[38;5;241m.\u001b[39mpages)\n\u001b[0;32m    102\u001b[0m     ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\pypdf\\_reader.py:332\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[1;34m(self, stream, strict, password)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[0;32m    331\u001b[0m         stream \u001b[38;5;241m=\u001b[39m BytesIO(fh\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_encryption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\pypdf\\_reader.py:1554\u001b[0m, in \u001b[0;36mPdfReader.read\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream: StreamType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_validation(stream)\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_eof_marker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1555\u001b[0m     startxref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_startxref_pos(stream)\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;66;03m# check and eventually correct the startxref only in not strict\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\pypdf\\_reader.py:1625\u001b[0m, in \u001b[0;36mPdfReader._find_eof_marker\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1624\u001b[0m         logger_warning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOF marker not found\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 1625\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[43mread_previous_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Anote\\lib\\site-packages\\pypdf\\_utils.py:268\u001b[0m, in \u001b[0;36mread_previous_line\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    266\u001b[0m found_crlf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     to_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(DEFAULT_BUFFER_SIZE, stream\u001b[38;5;241m.\u001b[39mtell())\n",
      "\u001b[1;31mPdfStreamError\u001b[0m: Stream has ended unexpectedly"
     ]
    }
   ],
   "source": [
    "results = run_eval(path_to_csv_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f529b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anote",
   "language": "python",
   "name": "anote"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
