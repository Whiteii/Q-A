{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08249901-79b4-40e3-82ca-c3a9d19205a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List\n",
    "import os \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336fa113-2f12-45b0-b251-e248414de161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2537bba2-c0c9-41eb-9f59-b86192c5b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f\"syntheses_df{count * 10}.csv\" for count in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8205bf68-193d-48d8-86b7-8c64e97eea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_l(candidate, reference):\n",
    "    m, n = len(candidate), len(reference)\n",
    "    #print(m,n)\n",
    "    dp_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if candidate[i - 1] == reference[j - 1]:\n",
    "                dp_table[i][j] = dp_table[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp_table[i][j] = max(dp_table[i - 1][j], dp_table[i][j - 1])\n",
    "    \n",
    "    return dp_table[m][n] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd271afb-97b9-4d6d-9230-fa6f3607a1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spurt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bd4160-df57-4b1a-8eb8-c0c85bc42047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1857a4d-ee23-41a7-9ae1-1b68197eaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af14816e-54ee-4922-91ad-934c0bd6f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_score(answer,syntheses):\n",
    "    tokens_answer = preprocess_text(answer)\n",
    "    tokens_syntheses = preprocess_text(syntheses)\n",
    "    str_answer = ' '.join(tokens_answer)\n",
    "    str_syntheses = ' '.join(tokens_syntheses)\n",
    "    freqdist_answer = nltk.FreqDist(str_answer.split())\n",
    "    freqdist_syntheses = nltk.FreqDist(str_syntheses.split())\n",
    "    # Extract frequencies for unique tokens in both texts\n",
    "    unique_tokens = set(freqdist_answer.keys()).union(freqdist_syntheses.keys())\n",
    "\n",
    "    freq_answer = [freqdist_answer[token] for token in unique_tokens]\n",
    "    freq_syntheses = [freqdist_syntheses[token] for token in unique_tokens]\n",
    "\n",
    "    vector_answer = np.array(freq_answer).reshape(1, -1)\n",
    "    vector_syntheses = np.array(freq_syntheses).reshape(1, -1)\n",
    "\n",
    "    similarity_score = cosine_similarity(vector_answer, vector_syntheses)[0][0]\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17528f5b-62d1-4101-8c9b-bd8fe225d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9caabc40-1a96-464a-a503-74cbb0d557f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokens_answer \u001b[38;5;241m=\u001b[39m preprocess_text(\u001b[43mdf_10\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m tokens_syntheses \u001b[38;5;241m=\u001b[39m preprocess_text(df_10[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyntheses\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_10' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_answer = preprocess_text(df_10['answer'][0])\n",
    "tokens_syntheses = preprocess_text(df_10['syntheses'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99c2d9-5e8c-42dc-823c-3a7e6ce32bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_answer(model_id, df):\n",
    "    query = \"How do I use the OpenAI API?\"\n",
    "    text_gen = pipeline(task=\"text-generation\", model=model_id, tokenizer=llama_tokenizer, max_length=200)\n",
    "    output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "    print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anote",
   "language": "python",
   "name": "anote"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
